AIra/
â”‚
â”œâ”€â”€ aira/
â”‚   â”œâ”€â”€ main.py              # FastAPI entry
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”œâ”€â”€ rag.py
â”‚   â”‚   â””â”€â”€ agent.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ llm_loader.py
â”‚   â”‚   â”œâ”€â”€ prompt_manager.py
â”‚   â”‚   â””â”€â”€ callbacks.py
â”‚   â”‚
â”‚   â”œâ”€â”€ chains/
â”‚   â”‚   â”œâ”€â”€ basic_chain.py
â”‚   â”‚   â””â”€â”€ rag_chain.py
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”‚   â””â”€â”€ tool_agent.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ search.py
â”‚   â”‚   â””â”€â”€ calculator.py
â”‚   â”‚
â”‚   â””â”€â”€ memory/
â”‚       â””â”€â”€ conversation.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ lora_finetune.py
â”‚   â”œâ”€â”€ qlora_finetune.py
â”‚   â””â”€â”€ configs/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ dvc.yaml
â”‚
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ vllm_server.py
â”‚   â””â”€â”€ client.py
â”‚
â”œâ”€â”€ mlflow/
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ pyproject.toml



- [x] Week 1: Core LLM Inference & Modular Architecture 
- [ ] Week 2: FastAPI-based Serving Layer(main.py should not call the llm directly)
   ðŸ“Œ What We Are NOT Doing in Week-2
    âŒ Auth
    âŒ RAG
    âŒ Memory
    âŒ Agents
    âŒ MLflow
    Version	Meaning
    0.1.0	Core LLM inference (Week 1)
    0.2.0	API serving layer (Week 2)
- [ ] Week 3: RAG with Vector Stores + DVC 
- [ ] Week 4: Agentic AI with Tools & Memory 
- [ ] Week 5: LoRA Fine-Tuning + MLflow 
- [ ] Week 6: QLoRA + Optimization 
- [ ] Week 7: vLLM Deployment 
- [ ] Week 8: Docker + CI/CD 
- [ ] Week 9+: Cloud Deployment & Monitoring


##### STEP-1 ######

* we use __init__.py file --> code in it run first as soon as the package is imported
* from pathlib import Path --> Imports a modern way to handle filesystem paths (better than strings).
* BASE_DIR = Path(__file__).parent.parent --> goes two levels back from the current script
* MODEL_DIR.mkdir(exist_ok=True, parents=True) --> parents=True makes sure the the parents dir exist
* we use @staticmethod inside class for any method in the class so that when we can use the 
  method directly withiout making any object from the class. it does not take self as a inut inside
  class, so it cannot interact with other methods or attributes inside class.
  e.g.--> prompt = PromptManager.get_basic_chat_prompt()
* What do you mean by prompt abstraction?
  In AIra, prompts are treated as first-class componentsâ€”defined independently from model 
  inference and orchestrated via LangChain PromptTemplatesâ€”so they can evolve with RAG, 
  agents, and fine-tuning without changing core logic


our current position(sha id: b1d656d-->we are getting hellusinated and unstructured outputs from 
                    the model)
    Raw LLM generation
    â†“
    Unbounded output
    â†“
    Hard to control
    â†“
    Not production-safe

    1. we are using Qwen/Qwen3-0.6B which is a text generation model
    2. raw inference, model don't know when to stop
    Raw inference â‰  Production inference


our current position(sha id: d67d6a0-->we are getting proper output with <think> as we are using
                    the template on which the qwen was trained on)
    Tokenizer-aware LLM abstraction
        â†“
    Qwen-native chat prompt alignment
        â†“
    LangChain BasicChain orchestration
        â†“
    EOS-driven bounded generation
        â†“
    Clean, deterministic responses



##### STEP-2 ######
 
    *currently our model is CLI(commad line interface) only